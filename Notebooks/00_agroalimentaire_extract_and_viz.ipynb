{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OSE Dataset Extraction and Visualization - Agroalimentaire Sector\n",
        "\n",
        "This notebook combines data extraction and visualization for companies in the **agroalimentaire** sector.\n",
        "\n",
        "**Workflow:**\n",
        "1. Load pre-processed `agroalimentaire.json` from `data/sector_slices/` (from sector_split notebook)\n",
        "2. Convert to JSONL format for pipeline compatibility\n",
        "3. Extract all 9 datasets using the fast pipeline\n",
        "4. Export datasets as CSV files to `data/extracted_datasets/`\n",
        "5. Load and visualize all datasets\n",
        "\n",
        "**Key advantage:** Uses pre-processed sector data, so you don't need to run the sector_split notebook anymore!\n",
        "\n",
        "**Input:** \n",
        "- Pre-processed `agroalimentaire.json` from `data/sector_slices/`\n",
        "- Article and project files from `data/new_data/`\n",
        "\n",
        "**Output:** \n",
        "- 9 CSV files in `data/extracted_datasets/`\n",
        "- Visualizations for each dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/jlb/Documents/Python Course/git_OSE/ose-main\n",
            "src exists: True\n",
            "Configuration loaded successfully!\n",
            "Sector slice file: ../src/ose_core/data/sector_slices/agroalimentaire.json\n",
            "New data directory: ../src/ose_core/data/new_data\n",
            "Output directory: ../src/ose_core/data/extracted_datasets\n",
            "Chunk size: 10000\n",
            "\n",
            "Verifying input files:\n",
            "  ✓ sector_slice: ../src/ose_core/data/sector_slices/agroalimentaire.json\n",
            "  ✓ company: ../src/ose_core/data/new_data/app_company.json\n",
            "  ✓ article: ../src/ose_core/data/new_data/app_article-003.json\n",
            "  ✓ project: ../src/ose_core/data/new_data/app_project.json\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "from typing import Dict, List, Any, Optional\n",
        "\n",
        "# Add project root to path so we can import from src\n",
        "cwd = Path(os.getcwd())\n",
        "\n",
        "# Check if we're in the project root (has src/ and Notebooks/ or notebooks/ directories)\n",
        "if (cwd / 'src').exists() and ((cwd / 'Notebooks').exists() or (cwd / 'notebooks').exists()):\n",
        "    project_root = cwd\n",
        "# Check if we're in Notebooks/ or notebooks/ directory\n",
        "elif (cwd.parent / 'src').exists() and ((cwd.parent / 'Notebooks').exists() or (cwd.parent / 'notebooks').exists()):\n",
        "    project_root = cwd.parent\n",
        "# Fallback: try relative path from Notebooks/\n",
        "else:\n",
        "    project_root = Path('..').resolve()\n",
        "\n",
        "sys.path.insert(0, str(project_root))\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"src exists: {(project_root / 'src').exists()}\")\n",
        "\n",
        "# Configuration - Data Sources (all under src/ose_core/data)\n",
        "SECTOR_SLICE_FILE = Path('../src/ose_core/data/sector_slices/agroalimentaire.json')\n",
        "NEW_DATA_DIR = Path('../src/ose_core/data/new_data')\n",
        "COMPANY_FALLBACK_PATH = NEW_DATA_DIR / 'app_company.json'\n",
        "company_path = str(COMPANY_FALLBACK_PATH)\n",
        "FILE_PATHS = {\n",
        "    'company': company_path,\n",
        "    'article': NEW_DATA_DIR / 'app_article-003.json',\n",
        "    'project': NEW_DATA_DIR / 'app_project.json'\n",
        "}\n",
        "# Store outputs inside the package data directory used by config\n",
        "OUTPUT_DIR = Path('../src/ose_core/data/extracted_datasets')\n",
        "CHUNK_SIZE = 10000  # Number of lines to process per chunk\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import extraction pipeline\n",
        "from src.ose_core.pipelines.extract_pipeline_v3_fast import make_extract_pipeline_v3_fast\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Configuration loaded successfully!\")\n",
        "print(f\"Sector slice file: {SECTOR_SLICE_FILE}\")\n",
        "print(f\"New data directory: {NEW_DATA_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Chunk size: {CHUNK_SIZE}\")\n",
        "\n",
        "# Verify files exist\n",
        "print(\"\\nVerifying input files:\")\n",
        "print(f\"  {'✓' if SECTOR_SLICE_FILE.exists() else '✗'} sector_slice: {SECTOR_SLICE_FILE}\")\n",
        "for file_type, file_path in FILE_PATHS.items():\n",
        "    # Convert to Path object if it's a string\n",
        "    path_obj = Path(file_path) if isinstance(file_path, str) else file_path\n",
        "    exists = path_obj.exists()\n",
        "    status = \"✓\" if exists else \"✗\"\n",
        "    print(f\"  {status} {file_type}: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Load Pre-processed Sector Data\n",
        "\n",
        "Load the pre-processed `agroalimentaire.json` file from sector_split and convert it to JSONL format for the extraction pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-processed sector data from ../src/ose_core/data/sector_slices/agroalimentaire.json...\n",
            "✓ Loaded 18,784 companies from sector slice\n",
            "Converting to JSONL format...\n",
            "✓ Temporary JSONL file created: /var/folders/5c/m3nm1tb53nj8kd1sy83fykk40000gn/T/tmp584edxvx.jsonl\n",
            "  Records written: 18,784\n",
            "\n",
            "Company file selection:\n",
            "  Temporary JSONL: /var/folders/5c/m3nm1tb53nj8kd1sy83fykk40000gn/T/tmp584edxvx.jsonl\n",
            "  Fallback path: ../src/ose_core/data/new_data/app_company.json\n",
            "  Using: /var/folders/5c/m3nm1tb53nj8kd1sy83fykk40000gn/T/tmp584edxvx.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Load pre-processed agroalimentaire.json and convert to JSONL format\n",
        "# Note: The sector slice may contain article data, so we'll check and use the actual company file\n",
        "print(f\"Loading pre-processed sector data from {SECTOR_SLICE_FILE}...\")\n",
        "\n",
        "temp_company_path = None\n",
        "use_sector_slice = False\n",
        "\n",
        "if SECTOR_SLICE_FILE.exists():\n",
        "    try:\n",
        "        # Load JSON array from sector slice file\n",
        "        with open(SECTOR_SLICE_FILE, 'r', encoding='utf-8') as f:\n",
        "            sector_data = json.load(f)\n",
        "\n",
        "        print(f\"✓ Loaded {len(sector_data):,} records from sector slice\")\n",
        "        \n",
        "        # Check if the sector slice contains company data (has socialName field)\n",
        "        # If it contains article data (has title field), we'll use the company file instead\n",
        "        if sector_data and isinstance(sector_data[0], dict):\n",
        "            first_record = sector_data[0]\n",
        "            has_company_fields = 'socialName' in first_record or 'siren' in first_record\n",
        "            has_article_fields = 'title' in first_record or 'contentClean' in first_record\n",
        "            \n",
        "            if has_company_fields and not has_article_fields:\n",
        "                # Sector slice contains company data - use it\n",
        "                use_sector_slice = True\n",
        "                print(\"  ✓ Sector slice contains company data - will use for extraction\")\n",
        "                \n",
        "                # Convert to JSONL format (one JSON object per line)\n",
        "                # Wrap in Elasticsearch format if needed\n",
        "                print(\"Converting to JSONL format...\")\n",
        "                with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False, encoding='utf-8') as tmp_file:\n",
        "                    for record in sector_data:\n",
        "                        # Ensure record is in Elasticsearch format with _source\n",
        "                        if '_source' not in record:\n",
        "                            es_record = {\n",
        "                                '_source': record,\n",
        "                                '_id': str(record.get('id', '')),\n",
        "                                '_index': 'company'\n",
        "                            }\n",
        "                        else:\n",
        "                            es_record = record\n",
        "                        json.dump(es_record, tmp_file, ensure_ascii=False)\n",
        "                        tmp_file.write('\\n')\n",
        "                    temp_company_path = tmp_file.name\n",
        "\n",
        "                print(f\"✓ Temporary JSONL file created: {temp_company_path}\")\n",
        "                print(f\"  Records written: {len(sector_data):,}\")\n",
        "            else:\n",
        "                # Sector slice contains article data - use company file instead\n",
        "                print(\"  ⚠ Sector slice contains article data, not company data\")\n",
        "                print(\"  → Will use full company file for extraction\")\n",
        "        else:\n",
        "            print(\"  ⚠ Could not determine data type in sector slice\")\n",
        "            print(\"  → Will use full company file for extraction\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading sector slice: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        temp_company_path = None\n",
        "else:\n",
        "    print(f\"⚠ Sector slice file not found: {SECTOR_SLICE_FILE}\")\n",
        "    print(\"  → Will use full company file for extraction\")\n",
        "\n",
        "# Choose company path: prefer sector slice if it contains company data, otherwise use company file\n",
        "if temp_company_path and use_sector_slice:\n",
        "    company_path = temp_company_path\n",
        "else:\n",
        "    company_path = str(COMPANY_FALLBACK_PATH)\n",
        "\n",
        "FILE_PATHS['company'] = company_path\n",
        "\n",
        "print(\"\\nCompany file selection:\")\n",
        "print(f\"  Sector slice JSONL: {temp_company_path if temp_company_path else 'not created'}\")\n",
        "print(f\"  Full company file: {COMPANY_FALLBACK_PATH}\")\n",
        "print(f\"  Using: {company_path}\")\n",
        "if use_sector_slice:\n",
        "    print(\"  → Using filtered sector slice data\")\n",
        "else:\n",
        "    print(\"  → Using full company file (recommended for complete extraction)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Extract Datasets\n",
        "\n",
        "Run the extraction pipeline to generate all 9 datasets from the filtered company data and related articles/projects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File paths prepared for extraction:\n",
            "  Company (from sector slice): /var/folders/5c/m3nm1tb53nj8kd1sy83fykk40000gn/T/tmp584edxvx.jsonl\n",
            "  Article: ../src/ose_core/data/new_data/app_article-003.json\n",
            "  Project: ../src/ose_core/data/new_data/app_project.json\n"
          ]
        }
      ],
      "source": [
        "# Prepare file paths for extraction pipeline\n",
        "extraction_file_paths = {\n",
        "    'company': company_path,\n",
        "    'article': str(FILE_PATHS['article']),\n",
        "    'project': str(FILE_PATHS['project'])\n",
        "}\n",
        "\n",
        "print(\"File paths prepared for extraction:\")\n",
        "if temp_company_path:\n",
        "    print(f\"  Company (from sector slice): {extraction_file_paths['company']}\")\n",
        "else:\n",
        "    print(f\"  Company (fallback): {extraction_file_paths['company']}\")\n",
        "print(f\"  Article: {extraction_file_paths['article']}\")\n",
        "print(f\"  Project: {extraction_file_paths['project']}\")\n",
        "\n",
        "# Verify files exist\n",
        "if not Path(extraction_file_paths['company']).exists():\n",
        "    print(f\"  ⚠ Warning: Company file not found: {extraction_file_paths['company']}\")\n",
        "if not Path(extraction_file_paths['article']).exists():\n",
        "    print(f\"  ⚠ Warning: Article file not found: {extraction_file_paths['article']}\")\n",
        "if not Path(extraction_file_paths['project']).exists():\n",
        "    print(f\"  ⚠ Warning: Project file not found: {extraction_file_paths['project']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing data through extraction pipeline...\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Processing company file: /var/folders/5c/m3nm1tb53nj8kd1sy83fykk40000gn/T/tmp584edxvx.jsonl\n",
            "============================================================\n",
            "Loading data from /var/folders/5c/m3nm1tb53nj8kd1sy83fykk40000gn/T/tmp584edxvx.jsonl in chunks of 10000...\n",
            "Total lines loaded: 18784\n",
            "Total chunks processed: 2, Total records: 18784\n",
            "\n",
            "============================================================\n",
            "Processing article file: ../src/ose_core/data/new_data/app_article-003.json\n",
            "============================================================\n",
            "Loading data from ../src/ose_core/data/new_data/app_article-003.json in chunks of 10000...\n",
            "  Processed 100000 lines...\n",
            "  Processed 10 chunks (100000 records)...\n",
            "  Processed 200000 lines...\n",
            "  Processed 20 chunks (200000 records)...\n",
            "  Processed 300000 lines...\n",
            "  Processed 30 chunks (300000 records)...\n",
            "  Processed 400000 lines...\n",
            "  Processed 40 chunks (400000 records)...\n",
            "  Processed 500000 lines...\n",
            "  Processed 50 chunks (500000 records)...\n",
            "  Processed 600000 lines...\n",
            "  Processed 60 chunks (600000 records)...\n",
            "  Processed 700000 lines...\n",
            "  Processed 70 chunks (700000 records)...\n",
            "  Processed 794553 lines...\n",
            "  Processed 80 chunks (794553 records)...\n",
            "Total lines loaded: 794553\n",
            "Total chunks processed: 80, Total records: 794553\n",
            "\n",
            "============================================================\n",
            "Processing project file: ../src/ose_core/data/new_data/app_project.json\n",
            "============================================================\n",
            "Loading data from ../src/ose_core/data/new_data/app_project.json in chunks of 10000...\n",
            "  Processed 100000 lines...\n",
            "  Processed 10 chunks (100000 records)...\n",
            "  Processed 200000 lines...\n",
            "  Processed 20 chunks (200000 records)...\n",
            "  Processed 300000 lines...\n",
            "  Processed 30 chunks (300000 records)...\n",
            "  Processed 400000 lines...\n",
            "  Processed 40 chunks (400000 records)...\n",
            "  Processed 500000 lines...\n",
            "  Processed 50 chunks (500000 records)...\n",
            "  Processed 600000 lines...\n",
            "  Processed 60 chunks (600000 records)...\n",
            "  Processed 700000 lines...\n",
            "  Processed 70 chunks (700000 records)...\n",
            "  Processed 800000 lines...\n",
            "  Processed 80 chunks (800000 records)...\n",
            "  Processed 900000 lines...\n",
            "  Processed 90 chunks (900000 records)...\n",
            "  Processed 1000000 lines...\n",
            "  Processed 100 chunks (1000000 records)...\n",
            "  Processed 1097448 lines...\n",
            "  Processed 110 chunks (1097448 records)...\n",
            "Total lines loaded: 1097448\n",
            "Total chunks processed: 110, Total records: 1097448\n",
            "\n",
            "✅ Extraction complete!\n",
            "   Datasets extracted: 9\n",
            "\n",
            "Extracted 9 datasets:\n",
            "  - 01_company_basic_info: (18784, 17)\n",
            "  - 02_financial_data: (18784, 12)\n",
            "  - 03_workforce_data: (18784, 8)\n",
            "  - 04_company_structure: (18784, 10)\n",
            "  - 05_classification_flags: (18784, 17)\n",
            "  - 06_contact_metrics: (18784, 10)\n",
            "  - 07_kpi_data: (empty)\n",
            "  - 08_signals: (2741487, 17)\n",
            "  - 09_articles: (907270, 15)\n"
          ]
        }
      ],
      "source": [
        "# Run extraction pipeline\n",
        "if extraction_file_paths:\n",
        "    print(f\"Processing data through extraction pipeline...\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    try:\n",
        "        # Run the fast extraction pipeline\n",
        "        datasets = make_extract_pipeline_v3_fast(\n",
        "            file_paths=extraction_file_paths,\n",
        "            chunk_size=CHUNK_SIZE\n",
        "        )\n",
        "\n",
        "        print(f\"\\n✅ Extraction complete!\")\n",
        "        print(f\"   Datasets extracted: {len(datasets)}\")\n",
        "\n",
        "        # Display dataset summary\n",
        "        print(f\"\\nExtracted {len(datasets)} datasets:\")\n",
        "        for name in sorted(datasets.keys()):\n",
        "            df = datasets[name]\n",
        "            if not df.empty:\n",
        "                print(f\"  - {name}: {df.shape}\")\n",
        "            else:\n",
        "                print(f\"  - {name}: (empty)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error during extraction: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        datasets = {}\n",
        "else:\n",
        "    print(\"❌ Cannot run extraction - file paths not prepared\")\n",
        "    datasets = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Export Datasets to CSV\n",
        "\n",
        "Export all extracted datasets as CSV files for later use and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exporting 9 datasets to CSV files...\n",
            "================================================================================\n",
            "  ✓ 01_company_basic_info.csv: 18,784 rows, 17 columns\n",
            "  ✓ 02_financial_data.csv: 18,784 rows, 12 columns\n",
            "  ✓ 03_workforce_data.csv: 18,784 rows, 8 columns\n",
            "  ✓ 04_company_structure.csv: 18,784 rows, 10 columns\n",
            "  ✓ 05_classification_flags.csv: 18,784 rows, 17 columns\n",
            "  ✓ 06_contact_metrics.csv: 18,784 rows, 10 columns\n",
            "  ⚠ Skipping 07_kpi_data.csv (empty)\n",
            "  ✓ 08_signals.csv: 2,741,487 rows, 17 columns\n",
            "  ✓ 09_articles.csv: 907,270 rows, 15 columns\n",
            "\n",
            "✅ All datasets exported to: ../src/ose_core/data/extracted_datasets\n",
            "   Total datasets exported: 8\n",
            "   Files are ready for visualization\n",
            "\n",
            "  Temporary file cleaned up: /var/folders/5c/m3nm1tb53nj8kd1sy83fykk40000gn/T/tmp584edxvx.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Export 9 datasets to CSV files\n",
        "if 'datasets' in locals() and datasets:\n",
        "    print(f\"Exporting {len(datasets)} datasets to CSV files...\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Expected dataset names in order\n",
        "    expected_datasets = [\n",
        "        '01_company_basic_info',\n",
        "        '02_financial_data',\n",
        "        '03_workforce_data',\n",
        "        '04_company_structure',\n",
        "        '05_classification_flags',\n",
        "        '06_contact_metrics',\n",
        "        '07_kpi_data',\n",
        "        '08_signals',\n",
        "        '09_articles'\n",
        "    ]\n",
        "\n",
        "    # Export each dataset\n",
        "    exported_count = 0\n",
        "    for name in expected_datasets:\n",
        "        if name in datasets:\n",
        "            df = datasets[name]\n",
        "            if df.empty:\n",
        "                print(f\"  ⚠ Skipping {name}.csv (empty)\")\n",
        "                continue\n",
        "\n",
        "            # Create filename: 01_company_basic_info.csv\n",
        "            csv_path = OUTPUT_DIR / f\"{name}.csv\"\n",
        "\n",
        "            # Export to CSV\n",
        "            df.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "            print(f\"  ✓ {name}.csv: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
        "            exported_count += 1\n",
        "        else:\n",
        "            print(f\"  ✗ {name}.csv: MISSING\")\n",
        "\n",
        "    print(f\"\\n✅ All datasets exported to: {OUTPUT_DIR}\")\n",
        "    print(f\"   Total datasets exported: {exported_count}\")\n",
        "    print(f\"   Files are ready for visualization\")\n",
        "\n",
        "    # Clean up temporary file\n",
        "    if 'temp_company_path' in locals() and temp_company_path:\n",
        "        try:\n",
        "            os.unlink(temp_company_path)\n",
        "            print(f\"\\n  Temporary file cleaned up: {temp_company_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n  ⚠ Could not clean up temporary file: {e}\")\n",
        "else:\n",
        "    print(\"❌ No datasets available for export\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Load Datasets for Visualization\n",
        "\n",
        "Load the exported CSV files for visualization and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets from CSV files...\n",
            "================================================================================\n",
            "  ✓ 01_company_basic_info: (18784, 17)\n",
            "  ✓ 02_financial_data: (18784, 12)\n",
            "  ✓ 03_workforce_data: (18784, 8)\n",
            "  ✓ 04_company_structure: (18784, 10)\n",
            "  ✓ 05_classification_flags: (18784, 17)\n",
            "  ✓ 06_contact_metrics: (18784, 10)\n",
            "  ✓ 08_signals: (2741487, 17)\n",
            "  ✓ 09_articles: (907270, 15)\n",
            "\n",
            "✅ Loaded 8 datasets\n",
            "   Total datasets: 9\n"
          ]
        }
      ],
      "source": [
        "# Load datasets from CSV files\n",
        "print(\"Loading datasets from CSV files...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "data = {}\n",
        "dataset_files = {\n",
        "    '01_company_basic_info': '01_company_basic_info.csv',\n",
        "    '02_financial_data': '02_financial_data.csv',\n",
        "    '03_workforce_data': '03_workforce_data.csv',\n",
        "    '04_company_structure': '04_company_structure.csv',\n",
        "    '05_classification_flags': '05_classification_flags.csv',\n",
        "    '06_contact_metrics': '06_contact_metrics.csv',\n",
        "    '07_kpi_data': '07_kpi_data.csv',\n",
        "    '08_signals': '08_signals.csv',\n",
        "    '09_articles': '09_articles.csv'\n",
        "}\n",
        "\n",
        "for name, filename in dataset_files.items():\n",
        "    filepath = OUTPUT_DIR / filename\n",
        "    if filepath.exists():\n",
        "        try:\n",
        "            df = pd.read_csv(filepath, low_memory=False)\n",
        "            data[name] = df\n",
        "            print(f\"  ✓ {name}: {df.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ {name}: Error loading - {e}\")\n",
        "            data[name] = pd.DataFrame()\n",
        "    else:\n",
        "        # File not found - likely skipped during export because it was empty\n",
        "        # Skip warning to avoid confusion (Part 3 already notified about skipped files)\n",
        "        data[name] = pd.DataFrame()\n",
        "\n",
        "print(f\"\\n✅ Loaded {len([k for k, v in data.items() if not v.empty])} datasets\")\n",
        "print(f\"   Total datasets: {len(data)}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ose-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
